<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://josecastillolema.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://josecastillolema.github.io/" rel="alternate" type="text/html" /><updated>2025-01-06T15:28:32-06:00</updated><id>https://josecastillolema.github.io/feed.xml</id><title type="html">GitOps</title><subtitle>Networks, SDN, NFVi, OpenStack, K8s/OpenShift.</subtitle><author><name>Jose Castillo Lema</name></author><entry><title type="html">Introduction to Backstage - Developer Portals Made Easy (LFS142)</title><link href="https://josecastillolema.github.io/lf-backstage/" rel="alternate" type="text/html" title="Introduction to Backstage - Developer Portals Made Easy (LFS142)" /><published>2025-01-05T00:00:00-06:00</published><updated>2025-01-05T00:00:00-06:00</updated><id>https://josecastillolema.github.io/lf-backstage</id><content type="html" xml:base="https://josecastillolema.github.io/lf-backstage/"><![CDATA[<p><a href="https://training.linuxfoundation.org/"><img src="/assets/images/posts/2021-09-03-lf-courses.png" alt="" /></a></p>

<p><a href="https://training.linuxfoundation.org/">Linux Foundation Training &amp; Certification</a> provides free online-learning courses on a range of open source topics from Linux to blockchain, networking to cloud, and everything in between, with the possibility of earning certificates and badges.</p>

<p>I would like to recommend the <a href="https://trainingportal.linuxfoundation.org/courses/introduction-to-backstage-developer-portals-made-easy-lfs142">Introduction to Backstage - Developer Portals Made Easy</a> course to learn how developer portals like <a href="https://backstage.io/">Backstage</a> can help teams reduce tech fragmentation, knowledge silos, and lack of ownership while promoting creativity and autonomy.</p>

<h1 id="lfs142">LFS142</h1>

<p>Earners of the <strong>LFS142: Introduction to Backstage - Developer Portals Made Easy</strong> badge have learned to use the Backstage framework to advocate for and implement developer portals that enhance productivity and creativity within organizations. They can explain Backstage’s architecture, features, and setup processes, tailor Backstage to meet specific organizational needs, and customize it with plugins. They know how to deploy a developer portal that fosters autonomy and reduces technical silos.</p>

<div data-iframe-width="500" data-iframe-height="270" data-share-badge-id="4ee91574-1fa0-4dbc-ba9f-4c86b32eee83" data-share-badge-host="https://www.credly.com"></div>
<script type="text/javascript" async="" src="//cdn.credly.com/assets/utilities/embed.js"></script>]]></content><author><name>Jose Castillo Lema</name></author><category term="cert" /><category term="en" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Getting Started with OpenTelemetry (LFS148)</title><link href="https://josecastillolema.github.io/lf-opentelemetry/" rel="alternate" type="text/html" title="Getting Started with OpenTelemetry (LFS148)" /><published>2025-01-04T00:00:00-06:00</published><updated>2025-01-04T00:00:00-06:00</updated><id>https://josecastillolema.github.io/lf-opentelemetry</id><content type="html" xml:base="https://josecastillolema.github.io/lf-opentelemetry/"><![CDATA[[![](/assets/images/posts/2021-09-03-lf-courses.png)](https://training.linuxfoundation.org/)

[Linux Foundation Training & Certification](https://training.linuxfoundation.org/) provides free online-learning courses on a range of open source topics from Linux to blockchain, networking to cloud, and everything in between, with the possibility of earning certificates and badges.

I would like to recommend the [Getting Started with OpenTelemetry](https://trainingportal.linuxfoundation.org/courses/getting-started-with-opentelemetry-lfs148/) course to learn how to use [OpenTelemetry](https://opentelemetry.io/) to build and manage unified observability.

# LFS148

Earners of the **LSF148: Getting Started with OpenTelemetry** badge have hands-on experience implementing observability in software applications using OpenTelemetry. They can instrument Python and Java applications for traces, metrics, and logs, using both automatic and manual techniques. They understand telemetry signals, can use OpenTelemetry APIs and SDKs, and configure the OpenTelemetry Collector to export data to backends like Jaeger and Prometheus.

<div data-iframe-width="450" data-iframe-height="270" data-share-badge-id="56d75314-b80c-4942-952a-8e0f23fe28ff" data-share-badge-host="https://www.credly.com"></div><script type="text/javascript" async src="//cdn.credly.com/assets/utilities/embed.js"></script>]]></content><author><name>Jose Castillo Lema</name></author><category term="cert" /><category term="en" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Playing with Podman crun backends: Wasm(Edge) and libkrun</title><link href="https://josecastillolema.github.io/podman-wasm-libkrun/" rel="alternate" type="text/html" title="Playing with Podman crun backends: Wasm(Edge) and libkrun" /><published>2025-01-02T00:00:00-06:00</published><updated>2025-01-02T00:00:00-06:00</updated><id>https://josecastillolema.github.io/podman-wasm-libkrun</id><content type="html" xml:base="https://josecastillolema.github.io/podman-wasm-libkrun/"><![CDATA[<p>Thanks to <a href="https://github.com/containers/crun">crun</a> we can run <a href="#wasm">WebAssembly (Wasm)</a> and <a href="#libkrun">libkrun</a> workloads in directly in Podman.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ podman info | grep crun -A 2
    name: crun
    package: crun-1.19.1-1.fc41.x86_64
    path: /usr/bin/crun
    version: |-
      crun version 1.19.1
      commit: 3e32a70c93f5aa5fea69b50256cca7fd4aa23c80
      rundir: /run/user/1000/crun
      spec: 1.0.0
      +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL
</code></pre></div></div>

<h2 id="wasm">Wasm</h2>

<p>WebAssembly (abbreviated Wasm) is a portable binary instruction format. It has gained popularity for its portability as a compilation target that enables deployment on the web for both client and server applications.</p>

<p>We can leverage the portability of Wasm to run Wasm workloads alongside Linux containers by combining crun and Podman. crun supports running Wasm workload by using <a href="https://wasmedge.org/">WasmEdge</a>, <a href="https://wasmtime.dev/">Wasmtime</a> or <a href="https://wasmer.io/">Wasmer</a> runtimes. WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud-native and edge applications.</p>

<p>To enable Wasm(Edge) applications through Podman in Fedora we need to:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ rpm-ostree install wasmedge crun-wasm
</code></pre></div></div>

<p>To run Wasm applications though Podman:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ podman --runtime /usr/bin/crun-wasm run -dp 8080:8080 --platform=wasi/wasm -t --rm server-with-wasm
</code></pre></div></div>

<h2 id="libkrun">libkrun</h2>

<p><a href="https://github.com/containers/libkrun">libkrun</a> is a dynamic library that allows programs to easily acquire the ability to run processes in a partially isolated environment using KVM Virtualization on Linux and HVF on macOS/ARM64.</p>

<p>It integrates a VMM (Virtual Machine Monitor, the userspace side of an Hypervisor) with the minimum amount of emulated devices required to its purpose, abstracting most of the complexity that comes from Virtual Machine management, offering users a simple C API.</p>

<p><img src="/assets/images/posts/2025-01-02-podman-wasm-libkrun/1.webp" alt="" /></p>

<p>libkrun enables <a href="https://virtee.io/the-case-for-confidential-workloads/">Confidential Workloads (CW)</a>, autonomous, mission specific workloads that run inside a dedicated Virtualization-based TEE. They may make use of a minimal kernel, or have one statically-linked to it (unikernel case), but they shouldn’t depend on any other binary components.</p>

<p>This model is useful to quickly run and deploy small container-based applications, typically with a single container. The driving factor for confidential workloads is quick startup time and reduced resource usage for higher density.</p>

<p>To leverage libkrun through Podman in Fedora we need to:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ rpm-ostree install libkrun
</code></pre></div></div>

<p>To use the libkrun backend though Podman:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ podman run --annotation=run.oci.handler=krun -dp 8080:8080 -t --rm server-without-wasm
</code></pre></div></div>

<h2 id="setup">Setup</h2>

<p>For the comparison, we have used a <a href="https://github.com/josecastillolema/wasmedge-server">Simple Rust HTTP server</a> from the <a href="https://github.com/second-state/wasmedge-rustsdk-examples">WasmEdge Rust SDK examples</a> modified to run with and without WasmEdge, running locally on my laptop:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">use</span> <span class="nn">std</span><span class="p">::</span><span class="nn">net</span><span class="p">::</span><span class="n">SocketAddr</span><span class="p">;</span>
<span class="k">use</span> <span class="nn">hyper</span><span class="p">::</span><span class="nn">server</span><span class="p">::</span><span class="nn">conn</span><span class="p">::</span><span class="n">Http</span><span class="p">;</span>
<span class="k">use</span> <span class="nn">hyper</span><span class="p">::</span><span class="nn">service</span><span class="p">::</span><span class="n">service_fn</span><span class="p">;</span>
<span class="k">use</span> <span class="nn">hyper</span><span class="p">::{</span><span class="n">Body</span><span class="p">,</span> <span class="n">Method</span><span class="p">,</span> <span class="n">Request</span><span class="p">,</span> <span class="n">Response</span><span class="p">,</span> <span class="n">StatusCode</span><span class="p">};</span>
<span class="k">use</span> <span class="nn">tokio</span><span class="p">::</span><span class="nn">net</span><span class="p">::</span><span class="n">TcpListener</span><span class="p">;</span>

<span class="cd">/// This is our service handler. It receives a Request, routes on its</span>
<span class="cd">/// path, and returns a Future of a Response.</span>
<span class="k">async</span> <span class="k">fn</span> <span class="nf">handle_request</span><span class="p">(</span><span class="n">req</span><span class="p">:</span> <span class="n">Request</span><span class="o">&lt;</span><span class="n">Body</span><span class="o">&gt;</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nb">Result</span><span class="o">&lt;</span><span class="n">Response</span><span class="o">&lt;</span><span class="n">Body</span><span class="o">&gt;</span><span class="p">,</span> <span class="nn">hyper</span><span class="p">::</span><span class="n">Error</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="k">match</span> <span class="p">(</span><span class="n">req</span><span class="nf">.method</span><span class="p">(),</span> <span class="n">req</span><span class="nf">.uri</span><span class="p">()</span><span class="nf">.path</span><span class="p">())</span> <span class="p">{</span>
        <span class="c1">// Serve some instructions at /</span>
        <span class="p">(</span><span class="o">&amp;</span><span class="nn">Method</span><span class="p">::</span><span class="n">GET</span><span class="p">,</span> <span class="s">"/"</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nf">Ok</span><span class="p">(</span><span class="nn">Response</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nn">Body</span><span class="p">::</span><span class="nf">from</span><span class="p">(</span>
            <span class="s">"Try POSTing data to /echo such as: `curl localhost:8080/echo -XPOST -d 'hello world'`"</span><span class="p">,</span>
        <span class="p">))),</span>

        <span class="c1">// Simply echo the body back to the client.</span>
        <span class="p">(</span><span class="o">&amp;</span><span class="nn">Method</span><span class="p">::</span><span class="n">POST</span><span class="p">,</span> <span class="s">"/echo"</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nf">Ok</span><span class="p">(</span><span class="nn">Response</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">req</span><span class="nf">.into_body</span><span class="p">())),</span>

        <span class="p">(</span><span class="o">&amp;</span><span class="nn">Method</span><span class="p">::</span><span class="n">POST</span><span class="p">,</span> <span class="s">"/echo/reversed"</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="p">{</span>
            <span class="k">let</span> <span class="n">whole_body</span> <span class="o">=</span> <span class="nn">hyper</span><span class="p">::</span><span class="nn">body</span><span class="p">::</span><span class="nf">to_bytes</span><span class="p">(</span><span class="n">req</span><span class="nf">.into_body</span><span class="p">())</span><span class="k">.await</span><span class="o">?</span><span class="p">;</span>

            <span class="k">let</span> <span class="n">reversed_body</span> <span class="o">=</span> <span class="n">whole_body</span><span class="nf">.iter</span><span class="p">()</span><span class="nf">.rev</span><span class="p">()</span><span class="nf">.cloned</span><span class="p">()</span><span class="py">.collect</span><span class="p">::</span><span class="o">&lt;</span><span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">u8</span><span class="o">&gt;&gt;</span><span class="p">();</span>
            <span class="nf">Ok</span><span class="p">(</span><span class="nn">Response</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nn">Body</span><span class="p">::</span><span class="nf">from</span><span class="p">(</span><span class="n">reversed_body</span><span class="p">)))</span>
        <span class="p">}</span>

        <span class="c1">// Return the 404 Not Found for other routes.</span>
        <span class="n">_</span> <span class="k">=&gt;</span> <span class="p">{</span>
            <span class="k">let</span> <span class="k">mut</span> <span class="n">not_found</span> <span class="o">=</span> <span class="nn">Response</span><span class="p">::</span><span class="nf">default</span><span class="p">();</span>
            <span class="o">*</span><span class="n">not_found</span><span class="nf">.status_mut</span><span class="p">()</span> <span class="o">=</span> <span class="nn">StatusCode</span><span class="p">::</span><span class="n">NOT_FOUND</span><span class="p">;</span>
            <span class="nf">Ok</span><span class="p">(</span><span class="n">not_found</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="nd">#[tokio::main(flavor</span> <span class="nd">=</span> <span class="s">"current_thread"</span><span class="nd">)]</span>
<span class="k">async</span> <span class="k">fn</span> <span class="nf">main</span><span class="p">()</span> <span class="k">-&gt;</span> <span class="nb">Result</span><span class="o">&lt;</span><span class="p">(),</span> <span class="nb">Box</span><span class="o">&lt;</span><span class="k">dyn</span> <span class="nn">std</span><span class="p">::</span><span class="nn">error</span><span class="p">::</span><span class="n">Error</span> <span class="o">+</span> <span class="nb">Send</span> <span class="o">+</span> <span class="nb">Sync</span><span class="o">&gt;&gt;</span> <span class="p">{</span>
    <span class="k">let</span> <span class="n">addr</span> <span class="o">=</span> <span class="nn">SocketAddr</span><span class="p">::</span><span class="nf">from</span><span class="p">(([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="mi">8080</span><span class="p">));</span>

    <span class="k">let</span> <span class="n">listener</span> <span class="o">=</span> <span class="nn">TcpListener</span><span class="p">::</span><span class="nf">bind</span><span class="p">(</span><span class="n">addr</span><span class="p">)</span><span class="k">.await</span><span class="o">?</span><span class="p">;</span>
    <span class="nd">println!</span><span class="p">(</span><span class="s">"Listening on http://{}"</span><span class="p">,</span> <span class="n">addr</span><span class="p">);</span>
    <span class="k">loop</span> <span class="p">{</span>
        <span class="k">let</span> <span class="p">(</span><span class="n">stream</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="n">listener</span><span class="nf">.accept</span><span class="p">()</span><span class="k">.await</span><span class="o">?</span><span class="p">;</span>

        <span class="nn">tokio</span><span class="p">::</span><span class="nn">task</span><span class="p">::</span><span class="nf">spawn</span><span class="p">(</span><span class="k">async</span> <span class="k">move</span> <span class="p">{</span>
            <span class="k">if</span> <span class="k">let</span> <span class="nf">Err</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">=</span> <span class="nn">Http</span><span class="p">::</span><span class="nf">new</span><span class="p">()</span><span class="nf">.serve_connection</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span> <span class="nf">service_fn</span><span class="p">(</span><span class="n">handle_request</span><span class="p">))</span><span class="k">.await</span> <span class="p">{</span>
                <span class="nd">println!</span><span class="p">(</span><span class="s">"Error serving connection: {:?}"</span><span class="p">,</span> <span class="n">err</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">});</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Dependencies with Wasm support look like:</p>
<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[dependencies]</span>
<span class="nn">hyper_wasi</span> <span class="o">=</span> <span class="p">{</span> <span class="py">version</span> <span class="p">=</span> <span class="s">"0.15"</span><span class="p">,</span> <span class="py">features</span> <span class="p">=</span> <span class="nn">["full"]</span><span class="p">}</span>
<span class="nn">tokio_wasi</span> <span class="o">=</span> <span class="p">{</span> <span class="py">version</span> <span class="p">=</span> <span class="s">"1"</span><span class="p">,</span> <span class="py">features</span> <span class="p">=</span> <span class="p">[</span><span class="s">"rt"</span><span class="p">,</span> <span class="s">"macros"</span><span class="p">,</span> <span class="s">"net"</span><span class="p">,</span> <span class="s">"time"</span><span class="p">,</span> <span class="s">"io-util"</span><span class="p">]}</span>
</code></pre></div></div>

<p>And without:</p>
<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[dependencies]</span>
<span class="nn">hyper</span> <span class="o">=</span> <span class="p">{</span> <span class="py">version</span> <span class="p">=</span> <span class="s">"0.14"</span><span class="p">,</span> <span class="py">features</span> <span class="p">=</span> <span class="nn">["full"]</span><span class="p">}</span>
<span class="nn">tokio</span> <span class="o">=</span> <span class="p">{</span> <span class="py">version</span> <span class="p">=</span> <span class="s">"1"</span><span class="p">,</span> <span class="py">features</span> <span class="p">=</span> <span class="p">[</span><span class="s">"rt"</span><span class="p">,</span> <span class="s">"macros"</span><span class="p">,</span> <span class="s">"net"</span><span class="p">,</span> <span class="s">"time"</span><span class="p">,</span> <span class="s">"io-util"</span><span class="p">]}</span>
</code></pre></div></div>

<p>The <a href="https://github.com/josecastillolema/wasmedge-server">repository</a> contains instructions to build, run, test and create container images for both scenarios.</p>

<h2 id="results">Results</h2>

<p>Let’s compare “normal” native containers, Wasm(Edge) and libkrun for several scenarios.</p>

<h3 id="with-podman">With podman</h3>

<h4 id="image-size">Image size</h4>

<p>Wasm image is 77% smaller than the usual container ones. To keep the without-wasm image minimal we have leveraged <a href="https://musl.libc.org/">musl</a>, a minimal C library that is often used on embedded systems and other environments where a full-featured library like glibc is not available.</p>

<p><img class="mermaid" src="https://mermaid.ink/svg/eyJjb2RlIjoieHljaGFydC1iZXRhXG50aXRsZSBcIkltYWdlIHNpemVcIlxueC1heGlzIFtwb2RtYW4sIHdhc21lZGdlLCBsaWJrcnVuXVxueS1heGlzIFwiU2l6ZSAoTUJzKVwiIDAgLS0-IDVcbmJhciBbNC45MywgMS4xNywgNC45M10iLCJtZXJtYWlkIjpudWxsfQ" /></p>

<h4 id="resource-usage">Resource usage</h4>

<h5 id="idle">Idle</h5>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ podman stats
ID            NAME        CPU %       MEM USAGE / LIMIT  MEM %       NET IO      BLOCK IO      PIDS        CPU TIME    AVG CPU %
241deb12adf5  podman      0.01%       208.9kB / 67.1GB   0.00%       0B / 726B   0B / 0B       1           3.915ms     0.01%
a568ddf0f97b  wasmedge    0.04%       26.35MB / 67.1GB   0.04%       0B / 796B   0B / 0B       1           30.242ms    0.04%
f906858667d5  libkrun     3.15%       101.5MB / 67.1GB   0.15%       0B / 656B   0B / 4.096kB  17          901.204ms   3.15%
</code></pre></div></div>

<h6 id="cpu">CPU</h6>

<p>libkrun presents a slightly higher CPU usage (and much more PIDs in use).</p>

<p><img class="mermaid" src="https://mermaid.ink/svg/eyJjb2RlIjoieHljaGFydC1iZXRhXG50aXRsZSBcIkNQVSB1c2FnZVwiXG54LWF4aXMgW3BvZG1hbiwgd2FzbWVkZ2UsIGxpYmtydW5dXG55LWF4aXMgXCJDUFUgKCUpXCIgMCAtLT4gNFxuYmFyIFswLjAxLCAwLjA0LCAzLjE1XSIsIm1lcm1haWQiOm51bGx9" /></p>

<h6 id="memory">Memory</h6>

<p>Native podman container memory consumption is less than a quarter MB. Wasmedge consumes over 25 MBs (x126) and libkrun over 100 MBs (x487).</p>

<p><img class="mermaid" src="https://mermaid.ink/svg/eyJjb2RlIjoieHljaGFydC1iZXRhXG50aXRsZSBcIk1lbW9yeSB1c2FnZVwiXG54LWF4aXMgW3BvZG1hbiwgd2FzbWVkZ2UsIGxpYmtydW5dXG55LWF4aXMgXCJNZW1vcnkgKE1CcylcIiAwIC0tPiAxMjBcbmJhciBbMC4yLCAyNi4zNSwgMTAxLjRdIiwibWVybWFpZCI6bnVsbH0" /></p>

<h5 id="under-load">Under load</h5>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ podman stats
ID            NAME        CPU %       MEM USAGE / LIMIT  MEM %       NET IO        BLOCK IO      PIDS        CPU TIME     AVG CPU %
241deb12adf5  podman      57.34%      417.8kB / 67.1GB   0.00%       0B / 1.216kB  0B / 0B       1           13.879976s   0.13%
110f88177027  wasmedge    98.57%      26.6MB  / 67.1GB   0.04%       0B / 656B     0B / 0B       1           6.130282s    27.35%
f906858667d5  libkrun     255.08%     109.1MB / 67.1GB   0.16%       0B / 1.146kB  0B / 4.096kB  17          1m4.485238s  0.66%
</code></pre></div></div>

<h6 id="cpu-1">CPU</h6>

<p>Native podman containers consume approximately half core. WasmEdge container takes almost a full core. Libkrun consumes over 2.5 cores.</p>

<p><img class="mermaid" src="https://mermaid.ink/svg/eyJjb2RlIjoieHljaGFydC1iZXRhXG50aXRsZSBcIkNQVSB1c2FnZVwiXG54LWF4aXMgW3BvZG1hbiwgd2FzbWVkZ2UsIGxpYmtydW5dXG55LWF4aXMgXCJDUFUgKCUpXCIgMCAtLT4gMzAwXG5iYXIgWzU3LCA5OCwgMjU1XSIsIm1lcm1haWQiOm51bGx9" /></p>

<h6 id="memory-1">Memory</h6>

<p>There are no major variations from the memory perspective compared to the idle scenario.</p>

<p><img class="mermaid" src="https://mermaid.ink/svg/eyJjb2RlIjoieHljaGFydC1iZXRhXG50aXRsZSBcIk1lbW9yeSB1c2FnZVwiXG54LWF4aXMgW3BvZG1hbiwgd2FzbWVkZ2UsIGxpYmtydW5dXG55LWF4aXMgXCJNZW1vcnkgKE1CcylcIiAwIC0tPiAxMjBcbmJhciBbMC40LCAyNi42LCAxMDkuMV0iLCJtZXJtYWlkIjpudWxsfQ" /></p>

<h4 id="networking-performance">Networking performance</h4>

<p>To compare networking performance among the three solutions we have used <a href="https://github.com/fcsonline/drill">Drill</a>, a HTTP load testing application written in Rust. The <a href="https://github.com/josecastillolema/wasmedge-server/blob/main/drill-benchmark.yml">benchmark file</a> can be found in the repository and generates 85 byte HTTP GETs and POSTs calls:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ drill --benchmark drill-benchmark.yml --stats
...
Time taken for tests      44.0 seconds
Total requests            2000000
Successful requests       2000000
Failed requests           0
Requests per second       45475.02 [#/sec]
Median time per request   0ms
Average time per request  0ms
Sample standard deviation 0ms
99.0'th percentile        0ms
99.5'th percentile        0ms
99.9'th percentile        1ms
</code></pre></div></div>

<h5 id="requests-per-second">Requests per second</h5>

<p>The WasmEdge podman container becomes unresponsive after some time. Even so, it only achieves a 4% rate compared with the native podman container solution.</p>

<p><img class="mermaid" src="https://mermaid.ink/svg/eyJjb2RlIjoieHljaGFydC1iZXRhXG50aXRsZSBcIlJlcXVlc3RzIHBlciBzZWNvbmRcIlxueC1heGlzIFtwb2RtYW4sIHdhc21lZGdlLCBsaWJrcnVuXVxueS1heGlzIFwiUlBTICgjL3NlYylcIiAwIC0tPiA4MDAwMFxuYmFyIFs2NTQ0OSwgMjY2MywgNDU0NzVdIiwibWVybWFpZCI6bnVsbH0" /></p>

<h5 id="average-time-per-request">Average time per request</h5>

<p>The WasmEdge podman container presents the worst latency even delivering only 4% of the request compared with the native container solution.</p>

<p><img class="mermaid" src="https://mermaid.ink/svg/eyJjb2RlIjoieHljaGFydC1iZXRhXG50aXRsZSBcIkF2ZXJhZ2UgdGltZSBwZXIgcmVxdWVzdFwiXG54LWF4aXMgW3BvZG1hbiwgd2FzbWVkZ2UsIGxpYmtydW5dXG55LWF4aXMgXCJBdmVyYWdlIHRpbWUgKG1zKVwiIDAgLS0-IDVcbmJhciBbMCwgNCwgMF0iLCJtZXJtYWlkIjpudWxsfQ" /></p>

<h4 id="summary">Summary</h4>

<p>The <strong>podman</strong> native container solution presented the best overall results as expected.</p>

<p><strong>libkrun</strong> has great network performance, and incurs in some resource overhead as expected of an hybrid virtualization solution. For networking, it implements <code class="language-plaintext highlighter-rouge">virtio-vsock+TSI</code>, an experimental mechanism that provides inbound and outbound networking capabilities to the guest, with zero-configuration and minimal footprint, by transparently replacing user-space <code class="language-plaintext highlighter-rouge">AF_INET</code> sockets with <code class="language-plaintext highlighter-rouge">AF_TSI</code>, implementing both <code class="language-plaintext highlighter-rouge">AF_INET</code> and <code class="language-plaintext highlighter-rouge">AF_VSOCK</code> sockets. TSI has the additional advantage that, for the host side, all connections appear to come and go to the process acting as a VMM, which makes it very container-friendly in a way that even side-cars (such as Istio) work out-of-the-box.</p>

<p>The <strong>WasmEdge</strong> solution presents a very acceptable resource consumption (and the best image size), however the networking performance is only a 4% compared to native podman and the application becomes unresponsive under heavy load.</p>

<p><img class="mermaid" src="https://mermaid.ink/svg/eyJjb2RlIjoicXVhZHJhbnRDaGFydFxudGl0bGUgU3VtbWFyeVxucXVhZHJhbnQtMSBSUFNcbnF1YWRyYW50LTIgTGF0ZW5jeVxucXVhZHJhbnQtMyBDUFVcbnF1YWRyYW50LTQgTWVtb3J5XG4lJSBycHNcbnBvZG1hbjogWzAuOTIsIDAuOTJdXG53YXNtZWRnZTogWzAuNTcsIDAuNTM2XVxubGlia3J1bjogWzAuOCwgMC44XVxuJSUgbGF0ZW5jeVxucG9kbWFuOiBbMC40LCAwLjldXG53YXNtZWRnZTogWzAuMDcsIDAuNTM2XVxubGlia3J1bjogWzAuMzUsIDAuODVdXG4lJSBDUFVcbnBvZG1hbjogWzAuNDIsIDAuNDJdXG53YXNtZWRnZTogWzAuMzUsIDAuMzVdXG5saWJrcnVuOiBbMC4xLCAwLjFdXG4lJSBtZW1vcnlcbnBvZG1hbjogWzAuOTIsIDAuNDJdXG53YXNtZWRnZTogWzAuOCwgMC4zXVxubGlia3J1bjogWzAuNTUsIDAuMDVdIiwibWVybWFpZCI6bnVsbH0" /></p>

<h3 id="without-podman">Without podman</h3>

<p>Wasm is often described as having “near-native performance”. Let’s remove podman from the equation to see if we observe a better networking performance.</p>

<h4 id="wasmedge">Wasmedge</h4>

<p>To build locally the WasmEdge enabled served:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ rustup target add wasm32-wasi
$ cargo build --target wasm32-wasi --release
</code></pre></div></div>

<p>Run the Wasm bytecode file in WasmEdge CLI.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ wasmedge target/wasm32-wasi/release/server-with-wasm.wasm
Listening on http://0.0.0.0:8080
</code></pre></div></div>

<p>And run the test:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ drill --benchmark drill-benchmark.yml --stats
...
Time taken for tests      91.4 seconds
Total requests            200000
Successful requests       200000
Failed requests           0
Requests per second       2188.77 [#/sec]
Median time per request   5ms
Average time per request  5ms
Sample standard deviation 2ms
99.0'th percentile        9ms
99.5'th percentile        10ms
99.9'th percentile        10ms
</code></pre></div></div>

<p>While the server did not become unresponsive and sustained a longer test, the RPS result is similar to the podman one.</p>

<h4 id="native">Native</h4>

<p>Compile the Rust server source code:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ rustup target add x86_64-unknown-linux-musl
$ cargo build --target x86_64-unknown-linux-musl --release
</code></pre></div></div>

<p>Run the server:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ./target/x86_64-unknown-linux-musl/release/server-without-wasm
Listening on http://0.0.0.0:8080
</code></pre></div></div>

<p>And run the test:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ drill --benchmark drill-benchmark.yml --stats
...
Time taken for tests      23.0 seconds
Total requests            2000000
Successful requests       2000000
Failed requests           0
Requests per second       86970.22 [#/sec]
Median time per request   0ms
Average time per request  0ms
Sample standard deviation 0ms
99.0'th percentile        0ms
99.5'th percentile        0ms
99.9'th percentile        0ms
</code></pre></div></div>

<p>We observe some podman overhead (approximately 30%).</p>

<h4 id="summary-1">Summary</h4>

<p>Podman is not responsible for the poor networking performance of the Wasmedge enabled server.</p>

<p><img class="mermaid" src="https://mermaid.ink/svg/eyJjb2RlIjoieHljaGFydC1iZXRhXG50aXRsZSBcIlJlcXVlc3RzIHBlciBzZWNvbmRcIlxueC1heGlzIFtwb2RtYW4sIG5hdGl2ZSwgcG9kbWFuLXdhc20sIHdhc21dXG55LWF4aXMgXCJSUFMgKCMvc2VjKVwiIDAgLS0-IDkwMDAwXG5iYXIgWzY1NDQ5LCA4Njk3MCwgMjY2MywgMjE4OF0iLCJtZXJtYWlkIjpudWxsfQ" /></p>

<p>Looking at this nice article about the <a href="https://00f.net/2023/01/04/webassembly-benchmark-2023/">Performance of WebAssembly runtimes in 2023</a> it does not look that Wasmedge is a slow runtime compared to the median performance. If you’re looking for the best performer, looks like <a href="https://github.com/bytecodealliance/wasm-micro-runtime">iwasm</a> is currently the one to choose, but overall Wasmtime, WasmEdge and Wasmer (supported by crun) are in the same ballpark and have a decent performance.</p>

<p>A quick look at the <code class="language-plaintext highlighter-rouge">wasmedge</code> CLI options shows an option that could be of interest for this particular use case, however no significant performance improvement was observed with it:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ wasmedge -h
...
--enable-threads
                Enable Threads proposal
...
</code></pre></div></div>

<h4 id="profiling">Profiling</h4>

<p>At this point, the performance hit looks to be due to 1) the Wasm runtime or 2) the Wasm ported libraries (<code class="language-plaintext highlighter-rouge">hyper_wasi</code> and <code class="language-plaintext highlighter-rouge">tokio_wasi</code>). Let’s take a closer look using flame graphs.</p>

<p>To profile a release build effectively we need to enable source line debug info. To do this, add the following lines to the <code class="language-plaintext highlighter-rouge">Cargo.toml</code> file:</p>
<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[profile.release]</span>
<span class="py">debug</span> <span class="p">=</span> <span class="mi">1</span>
</code></pre></div></div>

<p>Build and run again the server, clone the <a href="https://github.com/brendangregg/FlameGraph">FlameGraph repository</a> and for convenience put it on your path:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ perf record -F 99 -p [pid of server-with-wasm] --call-graph dwarf -- curl localhost:8080
$ perf script &gt; server-with-wasm.perf
$ stackcollapse-perf.pl server-with-wasm.perf &gt; server-with-wasm.folded
$ flamegraph.pl server-with-wasm.folded &gt; server-with-wasm.sv
</code></pre></div></div>

<p>This is the resulting flame graph. It looks like most of the time is spent on <code class="language-plaintext highlighter-rouge">libwasmedge.so</code> calls:
<img src="/assets/images/posts/2025-01-02-podman-wasm-libkrun/2.svg" alt="" /></p>]]></content><author><name>Jose Castillo Lema</name></author><category term="dev" /><category term="en" /><category term="redhat" /><summary type="html"><![CDATA[Thanks to crun we can run WebAssembly (Wasm) and libkrun workloads in directly in Podman. $ podman info | grep crun -A 2 name: crun package: crun-1.19.1-1.fc41.x86_64 path: /usr/bin/crun version: |- crun version 1.19.1 commit: 3e32a70c93f5aa5fea69b50256cca7fd4aa23c80 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL]]></summary></entry><entry><title type="html">Happy new year!</title><link href="https://josecastillolema.github.io/newyear25/" rel="alternate" type="text/html" title="Happy new year!" /><published>2025-01-01T00:00:00-06:00</published><updated>2025-01-01T00:00:00-06:00</updated><id>https://josecastillolema.github.io/newyear25</id><content type="html" xml:base="https://josecastillolema.github.io/newyear25/"><![CDATA[<p><img src="/assets/images/posts/2025-01-01-newyear25.png" alt="" /></p>]]></content><author><name>Jose Castillo Lema</name></author><category term="en" /><category term="es" /><category term="redhat" /><category term="pt" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">InstructLab: Democratizing AI Models at Scale (DL01001G)</title><link href="https://josecastillolema.github.io/instructlab/" rel="alternate" type="text/html" title="InstructLab: Democratizing AI Models at Scale (DL01001G)" /><published>2024-12-21T00:00:00-06:00</published><updated>2022-12-21T00:00:00-06:00</updated><id>https://josecastillolema.github.io/instructlab</id><content type="html" xml:base="https://josecastillolema.github.io/instructlab/"><![CDATA[<p><a href="https://www.ibm.com/training/"><img src="/assets/images/posts/2024-12-21-instructlab.jpg" alt="" /></a></p>

<p><a href="https://www.ibm.com/training/">IBM Training</a> provides free online-learning courses on a range of open source topics from Linux to blockchain, networking to cloud, and everything in between, with the possiblity of earning certificates and badges.</p>

<p>I would like to recommend the <a href="https://www.ibm.com/training/course/instructlab-democratizing-ai-models-at-scale-DL01001G">InstructLab: Democratizing AI Models at Scale</a> course which helps to understand InstructLab and its applications in customizing AI Language Models (SLMs and LLMs).</p>

<h1 id="course-overview">Course Overview</h1>

<p>Unlock the full potential of both Large Language Models (LLMs) and Small Language Models (SLMs) and transform your expertise into actionable insights. Have you ever felt that Language Models fall short in understanding your domain-specific knowledge? InstructLab bridges this gap, enabling you to contribute your expertise and enhance the model’s capabilities. This course will guide you through the process of democratizing AI with InstructLab, providing a deep dive into the underlying technology and hands-on tutorials to get you started. By the end of this course, you’ll be equipped to synthesize your domain specific data, fine-tune, align, and customize AI Language Models. Amplify your AI expertise, and earn a Credly digital badge to showcase your skills.</p>

<h1 id="course-details">Course details</h1>

<p>Objectives</p>

<ul>
  <li>Understand the LAB Methodology (Large-Scale Alignment for ChatBots) including taxonomy, synthetic data generation, and fine-tuning.</li>
  <li>Demonstrate InstructLab capabilities and its use cases.</li>
  <li>Contribute knowledge, skills, and the triage process for InstructLab through hands-on tutorials.</li>
  <li>Get InstructLab foundational badge through completing the course and passing the quiz with a minimum score of 80%.</li>
</ul>

<div data-iframe-width="450" data-iframe-height="270" data-share-badge-id="2c7e4052-7eee-4467-bdc6-d4695cb24d13" data-share-badge-host="https://www.credly.com"></div>
<script type="text/javascript" async="" src="//cdn.credly.com/assets/utilities/embed.js"></script>]]></content><author><name>Jose Castillo Lema</name></author><category term="cert" /><category term="en" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">OCaml (vs Python) y Unikernels con MirageOS</title><link href="https://josecastillolema.github.io/fp-madrid-ocaml/" rel="alternate" type="text/html" title="OCaml (vs Python) y Unikernels con MirageOS" /><published>2024-12-20T00:00:00-06:00</published><updated>2024-12-20T00:00:00-06:00</updated><id>https://josecastillolema.github.io/fp-madrid-ocaml</id><content type="html" xml:base="https://josecastillolema.github.io/fp-madrid-ocaml/"><![CDATA[<p><a href="https://www.meetup.com/es-ES/fp-madrid/events/305145445"><img src="/assets/images/posts/2024-12-20-fp-madrid-ocaml.png" alt="" /></a></p>

<p>Empezamos el nuevo año con <strong>OCaml</strong> como protagonista. Este lenguaje, a pesar de ser relativamente desconocido en nuestra comunidad, ha tenido un gran impacto en el diseño de lenguajes como Rust y Scala, también ha sido muy relevante en el mundo académico. En esta charla, José nos introducirá en el mundo de OCaml apoyándose en Python para hacer comparativas.</p>

<h2 id="abstract">Abstract</h2>
<p><a href="https://ocaml.org/">OCaml</a> es un lenguaje de programación funcional de propósito general y tipado estático, con inferencia de tipos, tipos inductivos, pattern matching, variantes, etc. además de ser muy eficiente. En esta charla veremos las principales características del lenguaje (comparadas con código Python equivalente) y hablaremos sobre el tooling y futuro del lenguaje y la posibilidad de crear (memory safe) unikernels usando la biblioteca de <a href="https://mirage.io/">MirageOS</a>.</p>

<h2 id="bio">Bio</h2>
<p>José trabaja como ingeniero de software en el equipo de Performance &amp; Scale de Red Hat, con tecnologías cloud-native y plataformas de orquestación de containers. Imparte cursos de Cloud, DevOps, Networking e IoT desde 2016 en cursos de grado y posgrado.</p>]]></content><author><name>Jose Castillo Lema</name></author><category term="dev" /><category term="es" /><category term="events" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Securing Your Software Supply Chain with Sigstore (LFS182)</title><link href="https://josecastillolema.github.io/lf-sigstore/" rel="alternate" type="text/html" title="Securing Your Software Supply Chain with Sigstore (LFS182)" /><published>2024-12-05T00:00:00-06:00</published><updated>2024-12-05T00:00:00-06:00</updated><id>https://josecastillolema.github.io/lf-sigstore</id><content type="html" xml:base="https://josecastillolema.github.io/lf-sigstore/"><![CDATA[<p><a href="https://training.linuxfoundation.org/"><img src="/assets/images/posts/2021-09-03-lf-courses.png" alt="" /></a></p>

<p><a href="https://training.linuxfoundation.org/">Linux Foundation Training &amp; Certification</a> provides free online-learning courses on a range of open source topics from Linux to blockchain, networking to cloud, and everything in between, with the possiblity of earning certificates and badges.</p>

<p>I would like to recommend the <a href="https://trainingportal.linuxfoundation.org/courses/securing-your-software-supply-chain-with-sigstore-lfs182">Securing Your Software Supply Chain with Sigstore</a> course which provides knowledge and skills necessary to secure the integrity of your software by leveraging the <a href="https://www.sigstore.dev/">Sigstore</a> toolkit, a free and open source project that offers automated signing and verification across release files, container images, binaries, bill of material manifests, and more.</p>

<h1 id="lfs182">LFS182</h1>

<p>Building and distributing software that is secure throughout its entire lifecycle can be challenging, leaving many projects unprepared to build securely by default. Attacks and vulnerabilities can emerge at any step of the chain, from writing to packaging and distributing software to end users.</p>

<p>Recently, several innovative technologies have emerged to improve the integrity of the software supply chain, reducing the friction developers face in implementing security within their daily work. <a href="https://www.sigstore.dev/">Sigstore</a>, a free and open source project, offers automated signing and verification across release files, container images, binaries, bill of material manifests, and more. These signed materials persist in a tamper-resistant publicly auditable log, so anyone can check for authenticity.</p>

<p>This course will provide you with the knowledge you need to build more securely by default, introducing you to some of the tools under Sigstore’s umbrella: Cosign, Fulcio, and Rekor. Once you complete this course, you will understand how to use Sigstore to secure your software development lifecycle.</p>

<p>By the end of this course, you should be able to:</p>
<ul>
  <li>Describe the components of Sigstore and how they support a more secure software supply chain.</li>
  <li>Be able to sign and verify software artifacts with Sigstore.</li>
  <li>Understand how to implement Sigstore within the software development lifecycle.</li>
</ul>

<div data-iframe-width="450" data-iframe-height="270" data-share-badge-id="e835f1aa-ece8-4c7a-ad64-a5bd91cc06bc" data-share-badge-host="https://www.credly.com"></div>
<script type="text/javascript" async="" src="//cdn.credly.com/assets/utilities/embed.js"></script>]]></content><author><name>Jose Castillo Lema</name></author><category term="cert" /><category term="en" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Red Hat OpenShift AI learning</title><link href="https://josecastillolema.github.io/openshift-ai-learning/" rel="alternate" type="text/html" title="Red Hat OpenShift AI learning" /><published>2024-08-02T00:00:00-05:00</published><updated>2024-08-02T00:00:00-05:00</updated><id>https://josecastillolema.github.io/openshift-ai-learning</id><content type="html" xml:base="https://josecastillolema.github.io/openshift-ai-learning/"><![CDATA[<p><a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-ai">OpenShift AI</a> gives data scientists and developers a powerful AI/ML platform for building AI-enabled applications. Data scientists and developers can collaborate to move from experiment to production in a consistent environment quickly.</p>

<div data-iframe-width="500" data-iframe-height="270" data-share-badge-id="858b6f70-edda-4c93-a117-e7b143f9cb0a" data-share-badge-host="https://www.credly.com"></div>
<script type="text/javascript" async="" src="//cdn.credly.com/assets/utilities/embed.js"></script>

<div data-iframe-width="500" data-iframe-height="270" data-share-badge-id="d3f502c8-e593-47f5-83c4-8517e35205f2" data-share-badge-host="https://www.credly.com"></div>
<script type="text/javascript" async="" src="//cdn.credly.com/assets/utilities/embed.js"></script>

<div data-iframe-width="500" data-iframe-height="270" data-share-badge-id="d7251aed-ea55-4e8e-840d-aab4d912924a" data-share-badge-host="https://www.credly.com"></div>
<script type="text/javascript" async="" src="//cdn.credly.com/assets/utilities/embed.js"></script>

<div data-iframe-width="500" data-iframe-height="270" data-share-badge-id="4648d588-be9e-4b16-8b73-f03e406023c0" data-share-badge-host="https://www.credly.com"></div>
<script type="text/javascript" async="" src="//cdn.credly.com/assets/utilities/embed.js"></script>

<div data-iframe-width="500" data-iframe-height="270" data-share-badge-id="5dad349e-abfd-4b07-87d3-d68c10523688" data-share-badge-host="https://www.credly.com"></div>
<script type="text/javascript" async="" src="//cdn.credly.com/assets/utilities/embed.js"></script>

<div data-iframe-width="500" data-iframe-height="270" data-share-badge-id="2ef45274-321f-42a7-bf91-be9703161fa4" data-share-badge-host="https://www.credly.com"></div>
<script type="text/javascript" async="" src="//cdn.credly.com/assets/utilities/embed.js"></script>]]></content><author><name>Jose Castillo Lema</name></author><category term="cert" /><category term="en" /><category term="iac" /><category term="openshift" /><category term="redhat" /><summary type="html"><![CDATA[OpenShift AI gives data scientists and developers a powerful AI/ML platform for building AI-enabled applications. Data scientists and developers can collaborate to move from experiment to production in a consistent environment quickly.]]></summary></entry><entry><title type="html">Unleashing the potential of Function as a Service in the cloud continuum</title><link href="https://josecastillolema.github.io/physics-rhr-blog/" rel="alternate" type="text/html" title="Unleashing the potential of Function as a Service in the cloud continuum" /><published>2024-06-05T00:00:00-05:00</published><updated>2024-06-05T00:00:00-05:00</updated><id>https://josecastillolema.github.io/physics-rhr-blog</id><content type="html" xml:base="https://josecastillolema.github.io/physics-rhr-blog/"><![CDATA[<p><a href="/physics"><img src="/assets/images/posts/2022-09-20-physics-ga4/1.png" alt="" /></a></p>

<p>Excited to share the article: <a href="https://research.redhat.com/blog/article/unleashing-the-potential-of-function-as-a-service-in-the-cloud-continuum/">Unleashing the potential of Function as a Service in the cloud continuum</a></p>

<p>The <a href="/physics">PHYSICS project</a> has successfully concluded, highlighting remarkable collaboration and innovation. Red Hat engineers and partners have made significant strides in evolving Function-as-a-Service (FaaS) in the cloud continuum, advancing multicluster automation, energy efficiency and autoscaling.</p>]]></content><author><name>Jose Castillo Lema</name></author><category term="en" /><category term="networks" /><category term="redhat" /><category term="research" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Management orchestrator for a Content-Centric Network in a 6G network</title><link href="https://josecastillolema.github.io/dnfa6/" rel="alternate" type="text/html" title="Management orchestrator for a Content-Centric Network in a 6G network" /><published>2024-05-20T00:00:00-05:00</published><updated>2024-05-20T00:00:00-05:00</updated><id>https://josecastillolema.github.io/dnfa6</id><content type="html" xml:base="https://josecastillolema.github.io/dnfa6/"><![CDATA[<p><a href="https://www.freepatentsonline.com/y2024/0163689.html"><strong>Management orchestrator for a Content-Centric Network in a 6G network</strong></a></p>

<h4 id="inventors">Inventors</h4>
<p>Jose Castillo Lema</p>

<h4 id="publication-date">Publication date</h4>
<p>2024/5/16</p>

<h4 id="patent-office">Patent office</h4>
<p>US</p>

<h4 id="application-number">Application number</h4>
<p>17985750</p>

<h4 id="description">Description</h4>
<p>A management orchestrator for a content-centric network in a 6G network is described herein according to some aspects. For example, the management orchestrator can receive, from a client device, a request for a network function in the content-centric network executed by a plurality of network nodes in the 6G network. In response to receiving the request for the network function, the management orchestrator can identify a particular network node of the plurality of network nodes usable for executing the network function. The management orchestrator can then automatically deploy a named function via the particular network node. The named function can be used to execute the network function for the client device.</p>

<h4 id="cite-this">Cite this</h4>

<h5 id="bibtex">BibTex</h5>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@{patent:20240163689,
 title     = "MANAGEMENT ORCHESTRATOR FOR A CONTENT-CENTRIC NETWORK IN A 6G NETWORK",
 number    = "20240163689",
 author    = "Castillo Lema, Jose (Sao Paulo, BR)",
 year      = "2024",
 month     = "May",
 url       = "https://www.freepatentsonline.com/y2024/0163689.html"
}
</code></pre></div></div>

<h5 id="endnote">EndNote</h5>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>*Patent
Author	Year	Title	Country	Assignee	Number	URL
Castillo Lema, Jose (Sao Paulo, BR)	2024	MANAGEMENT ORCHESTRATOR FOR A CONTENT-CENTRIC NETWORK IN A 6G NETWORK	United States	Red Hat, Inc. (Raleigh, NC, US)	20240163689	https://www.freepatentsonline.com/y2024/0163689.html
</code></pre></div></div>]]></content><author><name>Jose Castillo Lema</name></author><category term="ccn" /><category term="en" /><category term="networks" /><category term="nfv" /><category term="papers" /><category term="research" /><summary type="html"><![CDATA[Management orchestrator for a Content-Centric Network in a 6G network]]></summary></entry></feed>